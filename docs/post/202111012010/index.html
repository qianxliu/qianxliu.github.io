<!DOCTYPE html><head><title>概率论与统计推断（机器学习） - 兴趣使然的无名小站</title><meta name="description" content=""><script src="/bundle.js" onload="[].flat||(location='/update.html')"></script></head><main><article><h1>概率论与统计推断（机器学习）</h1><blockquote><p>统计推断（statistical inference），在计算机科学中也被称为“机器学习”，是使用数据推断生成数据分布的过程 一个经典的统计推断问题是：给一个样本($\sim$意味X_1,...,X_n独立且相互都有相同的边缘分布函数，即是来自F简单随机样本)$X_1,...,X_n \sim F$，如何推断$F?$</p></blockquote><h1 id="概率论与统计推断">概率论与统计推断</h1><h2 id="概率论基础">概率论基础</h2><h3 id="多变量分布与独立同分布样本">多变量分布与独立同分布样本</h3><p>令$X=(X_1,...X_n)$，其中$X_1,...X_n$均为随机变量。<b>这时，我们称$X$为一个随机向量(random vector)。</b> 如果$X_1,...,X_n$独立且相互都有相同的边缘分布函数$F$，我们称$X_1,...X_n$是独立同分布(IID, independent and identically distributed)的，同时使用如下记号表示：</p><p>$X_1,...,X_n \sim F$</p><p>如果分布$F$有密度函数$f$我们也写作$X_1,...,X_n \sim f$。我们也称$X_1,...,X_n$是来自$F$的大小为$n$的随机样本（注，国内一般称之为简单随机样本）。</p><p>此时由于独立性，</p><p>$P(X_1\in A_1, ..., X_n\in A_n) = \prod_{i=1}^{n} P(X_i\in A_i)，等价于f(x_1,...,x_n)=\prod_{i=1}^{n} f_{X_i}(x_i)$</p><p>独立同分布(IID, independent and identically distributed)样本具有相互独立且相同的多变量分布。 大部分统计推断理论和应用都是以独立同分布(IID)观测量为基础的。</p><h3 id="chernoff-bound">Chernoff Bound</h3><blockquote><p>一个可能的解释：Hoeffding 不等式是 Chernoff 界的推广，后者适用于 Bernoulli 随机变量，主要用于学习理论中(基于Wikipedia)。 另一个可能的解释：在学习理论(learning theory)中被称为Chernoff Bound，两者等价(基于CMU Eric Xing的ppt) 这两者的关系有一定考究的价值，在此不详谈</p></blockquote><p>Chernoff 界在计算学习理论中用于证明学习算法可能近似正确，即该算法在足够大的训练数据集上具有很小的误差的概率很高。因为其定义使用了独立同分布样本。</p><h4 id="矩母函数moment-generating-function">矩母函数(Moment Generating Function)</h4><p>$e^x$的麦克劳林公式定义为</p><p>$ e^x = \sum_{n=0}^{\infty}\frac{x^n}{n!} $</p><p>关于随机变量X的矩母函数，或者拉普拉斯变换，被定义为</p><p>$ M_X(t) = E(e^{tX}) = \int e^{tx}dF(x)$</p><p>t的取值范围为实数。</p><p>对t求导，从而可知</p><p>$ M_X^{(n)}(0) = E(e^{tX})^{(n)}|<em>{t=0} = E(X^ne^{{(tX)}^n})|</em>{t=0} = E(X^n)$</p><p>此处t取0.这是用导数方法的的出来的结论。而将$e^x$化为泰勒级数，可得</p><p>$ M_X^{(n)}(0) = E(e^{tX})^{(n)}|<em>{t=0} = E(\sum</em>{n=0}^{\infty}\frac{{(tX)}^n}{n!})^{(n)}|_{t=0} = E(X^n) $</p><p>有同样结果。拉普拉斯变换最玄妙之处在于其与泰勒级数的关系。</p><p><img src="https://img2020.cnblogs.com/blog/2051127/202111/2051127-20211107203908265-793748398.png" alt=""></p><p>只有在项中存在为${t^n}$才能n次求导到非0项，过大因为t=0的缘故亦为0.</p><h4 id="马尔可夫不等式（markovs-inequality）">马尔可夫不等式（Markov&#39;s inequality）</h4><p>令X是一个非负随机变量并且E(X)存在，则对于任意t&gt;0，有</p><p>$P(X&gt;t) \le \frac{E(X)}{t}$</p><p><b>证明：</b> 因为随机变量X&gt;0， $E(X) = \int_{0}^{\infty}xf(x)dx = \int_{0}^{t}xf(x)dx + \int_{t}^{\infty}xf(x)dx \ge \int_{t}^{\infty}xf(x)dx \ge t\int_{t}^{\infty}f(x)dx = tP(X&gt;t)$</p><h4 id="切比雪夫不等式（chebyshevs-inequality）">切比雪夫不等式（Chebyshev&#39;s inequality）</h4><p><b>证明：</b> 使用马尔可夫不等式: $P(|X-\mu| \ge t) = P({|X-\mu|}^2 \ge t^2) \le \frac{E(X-\mu)^2}{t^2} = \frac{\sigma^2}{t^2}$</p><h4 id="霍夫丁不等式（hoeffdings-inequality），切尔诺夫界chernoff-bound">霍夫丁不等式（Hoeffding&#39;s inequality），切尔诺夫界(Chernoff bound)</h4><p>霍夫丁不等式与马尔可夫不等式有共通之处，但霍夫丁不等式是一个更sharper的不等式。 $\epsilon$为常值epsilon符号。 霍夫丁不等式： $令Y_1,...,Y_n是独立观测值，使得E(Y_i)=0且a_i \le Y_i \le b_i.令\epsilon &gt; 0, 那么，对任意t&gt;0:$ $P(\sum_{i=1}^{n}Y_i \ge \epsilon) \le e^{-t\epsilon}\prod_{i=1}^{n}e^{t^2{(b_i-a_i)}^2/8}$ 另一个结论： $令X_1,...,X_n \sim B(p).那么, 对任意\epsilon &gt; 0,$ $P(|\overline{X}<em>n-p|&gt;\epsilon) \le 2e^{-2n{\epsilon}^2}$ $这里\overline{X}_n=\frac{\sum</em>{i=1}^{n}X_i}{n}$</p><h5 id="例子">例子</h5><p>令$X_1,...,X_n \sim B(p)$，即是来自$B(p)$的简单随机样本.令$n=100$且$\epsilon=0.2$。</p><p>我们使用切比雪夫不等式做第一次估计，因为切比雪夫本质上是Markov的变形，且使用方差而不是期望的性质，使得可以用于$B(p)$的估计。 $P(|\overline{X}_n-p|&gt;0.2) \le \frac{p(1-p)}{n{\epsilon}^2} \le \frac{1}{4n{\epsilon}^2} = 0.0625$ $P(|\overline{X}_n-p|&gt;0.2) \le 2e^{-2(100)(0.2^2)} = 0.00067$</p><p>可见，霍夫丁不等式更加sharper，n不断增大，霍夫丁不等式的减小速率更快且具有通用性。</p><h5 id="证明">证明</h5><h6 id="矩母函数的两个定理">矩母函数的两个定理</h6><p>$Theorem 4.2:$ 令X和Y为两个随机变量，如果 $M_X(t)=M_Y(t)$ 在所有$t \in (-\delta, \delta)(\delta &gt; 0)$上都成立，那么$X$和$Y$具有相同的分布。</p><p>$Theorem 4.3:$ 令X和Y为两个独立随机变量，那么$M_{X+Y}(t)=M_X(t)M_Y(t)$.</p><p>证明: $M_{X+Y}(t)=E[e^{t(X+Y)}]=E[e^{tX}e^{tY}]=E[e^{tX}]E[e^{tY}]=M_X(t)M_Y(t)$ $由于独立性，可得E[e^{tX}e^{tY}]=E[e^{tX}]E[e^{tY}]$</p><h6 id="切尔诺夫界的证明">切尔诺夫界的证明</h6><p>应用Markov不等式： 对于任意$t&gt;0$， $P(X \ge a) = P(e^{tX} \ge e^{ta}) \le \frac{E[e^{tX}]}{e^{ta}}$ 特别地， $P(X \ge a) \le \min_{t&gt;0}\frac{E[e^{tX}]}{e^{ta}}$ 相似地有对任意$t&lt;0$， $P(X \ge a) \le \min_{t&lt;0}\frac{E[e^{tX}]}{e^{ta}}$ 关键是最小化$\frac{E[e^{tX}]}{e^{ta}}$的$t$值。</p><h2 id="partial-reference">Partial Reference</h2><ul><li><a href="https://www.statlect.com/">https://www.statlect.com/</a></li><li><a href="https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459">https://towardsdatascience.com/the-poisson-distribution-and-poisson-process-explained-4e2cb17d459</a></li></ul><h1 id="changelog">ChangeLog</h1><ul><li>11月01日 19:38 写了第一部分。</li><li>11月03日 22:00 补充Reference和前面的部分。有一点困明天再写。明天要写Chernoff界相关内容，不可避免引入Markov不等式等概念。</li><li>11月07日 21:32 后面继续补充霍夫丁不等式和数理统计内容。有时间再写吧。</li><li>11月08日 19:51 书不在旁边明天再写吧。</li><li>11月12日 16:39 霍夫丁不等式补充了一点。晚上写完这部分。</li><li>12月1日 15:42 切尔诺夫界需要继续补充。等我仔细研读后再写。估计要将所有分布的属性要解释一遍。</li></ul></article></main>