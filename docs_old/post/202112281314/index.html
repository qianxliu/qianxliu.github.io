<!DOCTYPE html><head><title>Math-Based Approach on Neural Networks - 兴趣使然的无名小站</title><meta name="description" content=""><script src="/bundle.js" onload="[].flat||(location='/update.html')"></script></head><main><article><h1>Math-Based Approach on Neural Networks</h1><blockquote><p>自变量（Independent variable）一词来自数学。也叫实验刺激(inputs)。——qianxin</p></blockquote><h1 id="math-based-approach-on-neural-networks">Math-Based Approach on Neural Networks</h1><h2 id="perceptrons">Perceptrons</h2><h3 id="algebraic-terms">algebraic terms</h3><p>with inputs $x_1, x_2, ...$, weights $w_1, w_2, ...$, and bias $b$ is $ output=\left{\begin{matrix} 0\ if\ \sum_{j}w_jx_j \le thresold \ 1\ if\ \sum_{j}w_jx_j \gt thresold \end{matrix}\right. $</p><h3 id="dot-product-with-bias-as-thresold">dot product with bias as thresold</h3><p>$ output=\left{\begin{matrix} 0\ if\ w\cdot x+b \le 0 \ 1\ if\ w\cdot x+b \gt 0 \end{matrix}\right. $</p><h2 id="sigmoid-neuron">Sigmoid Neuron</h2><h3 id="sigmoid-function">Sigmoid Function</h3><p>$\sigma(z) \equiv \frac{1}{1+e^{-z}}$</p><p>with inputs $x_1, x_2, ...$, weights $w_1, w_2, ...$, and bias $b$ is</p><p>$output \equiv \sigma(z) \equiv \frac{1}{1+e^{-\sum_{j}w_jx_j-b}}$</p><h3 id="delta-output">$\Delta output$</h3><p>$\Delta{output}\approx \sum_{j}\frac{\partial output}{\partial w_j}\Delta{w_j}+\frac{\partial output}{\partial b}\Delta{b}$</p><h2 id="gradient-descent">Gradient descent</h2><h3 id="cost-function">Cost function</h3><p>In this formula, $y(x) \equiv output$, $C(w,b) \equiv \frac{1}{2n}\sum_{x}||y(x)-a||^2$</p><h1 id="changelog">ChangeLog</h1><ul><li>12月28日 12:59 1847年，柯西发明了梯度下降法</li></ul></article></main>