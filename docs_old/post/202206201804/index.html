<!DOCTYPE html><head><title>N-gram Language Models - 兴趣使然的无名小站</title><meta name="description" content=""><script src="/bundle.js" onload="[].flat||(location='/update.html')"></script></head><main><article><h1>N-gram Language Models</h1><h1 id="n-gram-language-models">N-gram Language Models</h1><p>We have an arbitrary sequence of m words.</p><h2 id="joint-possibility">Joint possibility</h2><p>$P(w_1, w_2, ..., w_m)$</p><h2 id="conditional-possibility">Conditional Possibility</h2><p>$P(w_1, w_2, ..., w_m)=P(w_1)P(w_2|w_1)P(w_3|w_1, w_2)...P(w_m|w_1, ..., w_{m-1})$</p><h2 id="markov-assumption">Markov Assumption</h2><p>A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.</p><p>$P(w_i|w_1, ..., w_{i-1})≈P(w_i|w_{i-n+1}...w_{i-1})$</p><h3 id="unigram-modeln1">Unigram Model(n=1)</h3><p>$P(w_1, w_2, ..., w_m)=\sum_{i=1}{m}P(w_i)$</p><h3 id="bigram-modeln2">Bigram Model(n=2)</h3><p>$P(w_1, w_2, ..., w_m)=\sum_{i=1}{m}P(w_i|w_{i-1})$</p><h3 id="trigram-modeln3">Trigram Model(n=3)</h3><p>$P(w_1, w_2, ..., w_m)=\sum_{i=1}{m}P(w_i|w_{i-2}w_{i-1})$</p></article></main>