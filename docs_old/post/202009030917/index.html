<!DOCTYPE html>
<!-- If <head> does not exist, some search engines will reject this page -->

<head>
  <title>Kaggle Machine Learning Tutorials(Chapter 4) - 兴趣使然的无名小站</title>
  <meta name="description" content="" />
  <!-- Firefox >= 62 | Chrome >= 69 | Safari >= 12 -->
  <script src="/bundle.js" onload="[].flat||(location='/update.html')"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.6.0/styles/atom-one-dark.min.css">
</head>
<main><article><h1>Kaggle Machine Learning Tutorials(Chapter 4)</h1><h1>前文提要</h1>
<p>上章我们建立了模型来拟合数据，这章我们将检验模型。</p>
<h1>Model-Validation(check accuracy,验证)</h1>
<p>You&#39;ve built a model. But how good is it?</p>
<p>In this lesson, you will learn to use model validation to measure the quality of your model. Measuring model quality is the key to iteratively improving your models.</p>
<h2>What is Model Validation</h2>
<p>You&#39;ll want to evaluate almost every model you ever build. In most (though not all) applications, the relevant measure(相关度量) of model quality is predictive accuracy. In other words, will the model&#39;s predictions be close(接近) to what actually happens.</p>
<p>Many people make a huge mistake when measuring predictive accuracy. They make predictions with their training data and compare those predictions to the target values in the training data. You&#39;ll see the problem with this approach(方法) and how to solve it in a moment, but let&#39;s think about how we&#39;d do this first.</p>
<p>You&#39;d first need to summarize the model quality into an understandable way. If you compare predicted and actual home values for 10,000 houses, you&#39;ll likely find mix of good and bad predictions. Looking through a list of 10,000 predicted and actual values would be pointless. We need to summarize this into a single metric(a system or standard of measurement).</p>
<p>There are many metrics for summarizing model quality, but we&#39;ll start with one called Mean Absolute Error (also called MAE). Let&#39;s break down this metric starting with the last word, error.</p>
<p>The prediction error for each house is:</p>
<pre><code class="hljs language-bash">error=actual−predicted
</code></pre><p>So, if a house cost $150,000 and you predicted it would cost $100,000 the error is $50,000.</p>
<p>With the MAE metric, we take the absolute value of each error. This converts each error to a positive number. We then take the average of those absolute errors. This is our measure of model quality. In plain English, it can be said as</p>
<pre><code class="hljs language-bash">On average, our predictions are off by about X.
</code></pre><p>To calculate MAE, we first need a model. That is built in a hidden cell below, which you can review by clicking the code button.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># Data Loading Code Hidden Here</span>
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

<span class="hljs-comment"># Load data</span>
melbourne_file_path = <span class="hljs-string">&#x27;../input/melbourne-housing-snapshot/melb_data.csv&#x27;</span>
melbourne_data = pd.read_csv(melbourne_file_path) 
<span class="hljs-comment"># Filter rows with missing price values</span>
filtered_melbourne_data = melbourne_data.dropna(axis=<span class="hljs-number">0</span>)
<span class="hljs-comment"># Choose target and features</span>
y = filtered_melbourne_data.Price
melbourne_features = [<span class="hljs-string">&#x27;Rooms&#x27;</span>, <span class="hljs-string">&#x27;Bathroom&#x27;</span>, <span class="hljs-string">&#x27;Landsize&#x27;</span>, <span class="hljs-string">&#x27;BuildingArea&#x27;</span>, 
                        <span class="hljs-string">&#x27;YearBuilt&#x27;</span>, <span class="hljs-string">&#x27;Lattitude&#x27;</span>, <span class="hljs-string">&#x27;Longtitude&#x27;</span>]
X = filtered_melbourne_data[melbourne_features]

<span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor
<span class="hljs-comment"># Define model</span>
melbourne_model = DecisionTreeRegressor()
<span class="hljs-comment"># Fit model</span>
melbourne_model.fit(X, y)
</code></pre><pre><code class="hljs language-bash">DecisionTreeRegressor(criterion=<span class="hljs-string">&#x27;mse&#x27;</span>, max_depth=None, max_features=None,
                      max_leaf_nodes=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      presort=False, random_state=None, splitter=<span class="hljs-string">&#x27;best&#x27;</span>)
</code></pre><p>Once we have a model, here is how we calculate the mean absolute error:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_absolute_error

predicted_home_prices = melbourne_model.predict(X)
mean_absolute_error(y, predicted_home_prices)
</code></pre><pre><code class="hljs language-bash">434.71594577146544
</code></pre><h2>The Problem with &quot;In-Sample&quot; Scores</h2>
<p>The measure we just computed can be called an &quot;in-sample&quot; score. We used a single &quot;sample&quot; of houses for both building the model and evaluating it. Here&#39;s why this is bad.</p>
<p>Imagine that, in the large real estate market, door color is unrelated to home price.</p>
<p>However, in the sample of data you used to build the model, all homes with green doors were very expensive. The model&#39;s job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.</p>
<p>Since this pattern was derived(派生) from the training data, the model will appear accurate in the training data.</p>
<p>But if this pattern doesn&#39;t hold when the model sees new data, the model would be very inaccurate when used in practice.</p>
<p>Since models&#39; practical(实际) value(价值) come from making predictions on new data, we measure performance on data that wasn&#39;t used to build the model. The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model&#39;s accuracy on data it hasn&#39;t seen before. This data is called validation data.</p>
</article></main>